{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence classification with Neural Networks\n",
    "## Part 3: Basic RNN model\n",
    "\n",
    "Now we're going to try RNN model (as GRU) on our time series data.\n",
    "The difference here is that we can feed the network with the whole sequence at once, so that it can learn the patterns and hopefully demonstrate better performance in presence of outliers.\n",
    "\n",
    "That should be relatively easy for our data. Basically the model could learn that:\n",
    "* speed of 5 can only happen at the begininng\n",
    "* or after the train segment speed has reached 0.\n",
    "\n",
    "If the speed of 5 (km/h) happens abruptly after any other speed value -- that would mean it's still a train segment. That means the network should be able to demonstrate high performance even with the 50% or more outliers in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "import altair as alt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from tmdprimer.datagen import generate_sample, Dataset, Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to create a shallow RNN architecture with just one recurrent layer and one output dense unit. But that should be enough for our case given simplicity of our data.\n",
    "\n",
    "The learning rate is adjusted with a schedule for faster convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# converge faster\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        0.01,\n",
    "        decay_steps=100,\n",
    "        decay_rate=0.7)\n",
    "\n",
    "def get_rnn_model():\n",
    "    rnn_model = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.GRU(8, return_sequences=True),\n",
    "            tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "        ]\n",
    "    )\n",
    "    rnn_model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=tf.keras.optimizers.RMSprop(learning_rate=lr_schedule),\n",
    "        metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    "    )\n",
    "    return rnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "0.05\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "data_rnn = []\n",
    "for outlier_prob in (0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0):\n",
    "    print(outlier_prob)\n",
    "    dataset = Dataset.generate(train_outlier_prob=outlier_prob, n_samples=120)\n",
    "    # truncate samples since we don't use masking and padding here\n",
    "    min_sample_size = min([len(s) for s in dataset.samples])\n",
    "    test_samples = [Sample(s.features[:min_sample_size]) for s in dataset.samples[100:]]\n",
    "    dataset.samples = [Sample(s.features[:min_sample_size]) for s in dataset.samples[:100]]\n",
    "    \n",
    "    model = get_rnn_model()\n",
    "\n",
    "    model.fit(\n",
    "        x=dataset.to_tfds(),\n",
    "        epochs=10,\n",
    "        verbose=0\n",
    "    )\n",
    "    dataset.samples = test_samples\n",
    "    res = model.evaluate(dataset.to_tfds(), verbose=0)\n",
    "    data_rnn.append({'outlier_prob': outlier_prob, 'accuracy': res[1]})\n",
    "    \n",
    "df_rnn = pd.DataFrame(data_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-7546021792d147bb8b82f5af01267f2c\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-7546021792d147bb8b82f5af01267f2c\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-7546021792d147bb8b82f5af01267f2c\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-737c8ca7e0633028d0b7c61d9bb14f25\"}, \"mark\": \"line\", \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"outlier_prob\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"accuracy\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-737c8ca7e0633028d0b7c61d9bb14f25\": [{\"outlier_prob\": 0.01, \"accuracy\": 0.9757462739944458}, {\"outlier_prob\": 0.05, \"accuracy\": 0.9686594009399414}, {\"outlier_prob\": 0.1, \"accuracy\": 0.9654850959777832}, {\"outlier_prob\": 0.2, \"accuracy\": 0.9653225541114807}, {\"outlier_prob\": 0.3, \"accuracy\": 0.9634259343147278}, {\"outlier_prob\": 0.4, \"accuracy\": 0.9636764526367188}, {\"outlier_prob\": 0.5, \"accuracy\": 0.9504933953285217}, {\"outlier_prob\": 0.6, \"accuracy\": 0.9407986402511597}, {\"outlier_prob\": 0.7, \"accuracy\": 0.9360632300376892}, {\"outlier_prob\": 0.8, \"accuracy\": 0.9166666865348816}, {\"outlier_prob\": 0.9, \"accuracy\": 0.900882363319397}, {\"outlier_prob\": 1.0, \"accuracy\": 0.4713917672634125}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt.Chart(df_rnn).mark_line().encode(x='outlier_prob', y='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right in line with our predictions, the model can easily learn the patterns in the data, and can yield over 90% accuracy even in case of whopping 80% of outliers.\n",
    "\n",
    "As expected, at the 100% outlier level, when features become indistinguishable, the network falls to a random 50% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see now how the tensorboard graphs look like for RNN. You can use those graphs as a reference when comparing them to the more complex models in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5/5 [==============================] - 2s 141ms/step - loss: 0.6486 - binary_accuracy: 0.6920\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 0.5264 - binary_accuracy: 0.8889\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 0.3011 - binary_accuracy: 0.9243\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.2226 - binary_accuracy: 0.9382\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 0.1922 - binary_accuracy: 0.9449\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 0.1705 - binary_accuracy: 0.9501\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 1s 103ms/step - loss: 0.1547 - binary_accuracy: 0.9537\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.1419 - binary_accuracy: 0.9571\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.1300 - binary_accuracy: 0.9607\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 88ms/step - loss: 0.1206 - binary_accuracy: 0.9625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1537aaa60>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clear any logs from previous runs\n",
    "tf.executing_eagerly()\n",
    "from datetime import datetime\n",
    "!rm -rf ./logs/\n",
    "log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "dataset = Dataset.generate(train_outlier_prob=0.10, n_samples=200)\n",
    "# truncate samples since we don't use masking and padding here\n",
    "min_sample_size = min([len(s) for s in dataset.samples])\n",
    "dataset.samples = [Sample(s.features[:min_sample_size]) for s in dataset.samples[:100]]\n",
    "\n",
    "get_rnn_model().fit(\n",
    "    x=dataset.to_tfds(),\n",
    "    epochs=10,\n",
    "    callbacks=[tensorboard_callback]\n",
    ")\n",
    "\n",
    "#%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
